{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXI. NP-COMPLETE PROBLEMS\n",
    "\n",
    "### Ubiquitous Intractability\n",
    "And for many important computational problems, including ones you are likely to run into in your own projects, there is no known efficient, let alone blazingly fast, algorithm for solving the problem.\n",
    "\n",
    "In the same way your program or toolbox should include lots of different design paradigms like dynamic programming, greedy algorithms, divide and conquer, lots of different data structures, hash tables, balance search trees, and so on. That toolbox also should include the knowledge of computationally intractable that is np complete problems. Maybe how to even establish problems are np complete. Again this is because, these problems are likely to show up in your own projects. \n",
    "\n",
    "### Polynomial-Time Solvability\n",
    "So rather than starting out by formalizing computational intractability, it's a little simpler to begin by defining computation tractability. That brings us to the uber important definition of polynomial time solvability. So, the problem is polymer time solubility naturally if there's a polynomial algorithm that solves it. That is there's an algorithm and there's a constant K, so that if you feed in an input of length n to this algorithm then it will correctly solve the problem. <br>\n",
    "So to keep the discussion simple, when we discuss computational intractability let's just think only about the deterministic algorithms. But at the same time, don't worry really all of the qualitative conclusions we're going to reach are believed to hold equally well for randomized algorithms. In particular we don't think that there are problems that deterministic algorithms require exponential time and yet randomization allows you to magically get polynomial time. There's even some mathematical evidence suggesting that is the case.\n",
    "\n",
    "### The Class P\n",
    "So that brings us to the definition of the complexity class capital P. Capital P is just defined as the set of all polynomials times solvable problems. The set all problems that admits a polynomial time algorithm solving it. Problems that are believed not to be in P, generally requires significant computational resources, significant human resources and a lot of domain expertise to solve and practice.\n",
    "\n",
    "\n",
    "### Reductions and Completeness\n",
    "How do we amass evidence of computational intractability, something like the traveling salesman problem. They key idea is through the formalism of completeness, which in turn depends on the idea of a reduction between two problems. Edmond's prescient conjecture from 1965 is in his paper Paths, Trees, and Flowers, when, while pondering the traveling salesman problem, he conjectures that there is no polynomial time algorithm for it. That conjecture is equivalent, as we'll see, to a conjecture called P not equal to NP, and this is not resolved. This remains to this day, one of the most fundamental open problems in all of mathematics.\n",
    "\n",
    "       P ≠ NP\n",
    "\n",
    "When we first introduced the all paired shortest path problem, we observed that one solution, and in certain cases is actually a very good solution, is to just invoke a single-source shortest path subroutine, like Dijkstra's Algorithm or the Bellman-Ford algorithm, n times, once for each choice of the source. But again, it's still a reduction given an efficient algorithm, a polynomial time algorithm. For the single-source shortest path problem, you just run it n times, and that gives you a polynomial type algorithm for the all-pairs shortest path. Math problem. In fact, seasoned algorithm designers, and seasoned programmers, are always looking for opportunities to employ reductions. <br>\n",
    "So somehow, the light side of the force. It spreads the frontier of ability to cover the things we can do with algorithms wider and wider. Now, for NP completeness, for establishing computational intractability we have to turn this world on its head. This relies on a perverse use of reductions.\n",
    "\n",
    "### The Class NP  ( nondeterministic polynomial )\n",
    "The definition of the complexity class NP as computational problems where you can efficiently recognize the solution, roughly the same thing is problem solvable is in brute force search. \n",
    "\n",
    "\n",
    "###  NP-Completeness  \n",
    "The idea of NP completeness, that the problem can be as hard as any other problem, where you can efficiently recognize a solution, the ubiquity of NP-complete problems, including possibly a problem you're working on right now in one of your own projects, and what should be part of your toolbox, a simple recipe for proving that problems are NP-complete. \n",
    "\n",
    "So, perhaps we can instead of mass evidence of interactability for the TSP by showing it's as hard as any other problem solvable using brute-force search, that is every other brute-force search solvable problem reduces to solving the traveling salesman problem. To turn this idea into mathematics we have to identify the minimal ingredients necessary to solve a problem using brute force search. And the key idea turns out to be theefficient recognition of purported solution. Specifically, we say that a computational problem, like, say the traveling salesman problem, or frankly almost any other problem we'vetalked about in these classes is a member of the complexity class NP if it meets the following two criteria: <br>\n",
    "- The first property is sort of a simple prerequisite. It is search that solutions have length polynomial in the input size. \n",
    "- The second property, and this is really the key one, is that if someone offers up to you a purported solution to a NP problem, you can verify its correctness in polynomial time.\n",
    "\n",
    "The complexity class NP would represent problems that are brute force solvable, in the same way that the traveling salesman problem is. If you look at the actual definition I just gave you of the class NP, it's in terms of efficient recognition of solutions, but let's now observe that this definition NP, in terms of efficientverification of purported solutions does indeed imply that these problems can be solved in exponential time using brute-force search. So really, all you do here is check each of the candidate solutions one at a time. And in doing so, you see very clearly the role of the two properties in thedefinition of an NP problem. The role of the first property saying solution length has to be a polynomial in the input size, in turn, restricts the number of possible solutions, the number of bit strings of that given length, to be at most, exponential in the input size. The second property of course assures you that for each of those only exponentially many possibilities, you can verify whether or not it is a correct solution, whether or not it's a short TSB tour, whether or not it's a satisfying assignment to some variables in a constraint satisfaction problem in polynomial time. Now because the requirements for the class NP are so weak, the class NP is very big, where allmembership in NP requires essentially as the ability to officially recognize the solution. You know one when you see one, and as you can imagine, that property is met by many of the computational problems that we ever think about. Factoring almost any graph problems you'dever think about, sequencing problems, most constraint satisfaction problems, and so on. As we've said, it's true that not every, natural computational problem, is NP. The halting problem is an extreme example. That's in fact undecidable, there's no algorithm at all. There's also somenatural problems and some application domains which are neither in NP, nor they undecidable. So, model checking is one domain that furnishes lots of examples of problems strictly harder than NP. The vastness of the complexity class NP means that NP-completeness is strong evidence of intractability. Remember what it means for a problem to be complete. For some set of problems, it means it's as hard as any other problem in that set. Every other problem in that set reduces to the complete problem. Therefore, suppose you had a polynomial time algorithm for just one, NP-complete problem, you would get, automatically, by the definition of a reduction, a polynomialtime algorithm for every single computational problem in NP. Every single problem for which you can efficiently recognize a solution. That is, solving just 1 NP-complete problem in polynomial time would imply that NP is the same as P. Then, every problem in NP can be solved in polynomial time.\n",
    "\n",
    "### The P vs. NP Question\n",
    "So how come so many people thing that P is not equal to NP, rather than the opposite, P=NP? Well, I think thedominant reason is sort of a psychological reason, namely that if it were the case that P=NP, then all you'd have to do, remember, is exhibit a polynomial time algorithm for just 1, np complete problem. And there are tons of np complete problems. And a lot of extremely smart people, have had np complete problems that they've really cared about, and either on purpose or accidentally, they've been trying to develop efficent algorithms for them. No one has ever succeeded in over 1/2 century of serious computational work. <br>\n",
    "Now, maybe it seems bizarre to you, that we're struggling to prove that P is notequal to NP. Maybe it just seems sort of obvious that there is no way that you can always construct proofs, in time, polynomial, in what you need to verify proofs. But, the reason this is so hard to prove mathematically, is because of the insane richness of the space of polynomial timealgorithms. And indeed, it's this richness that we've been exploiting all along in these design and analyses of algorithms classes. Think for example about matrix multiplication. Had we not seen the Strassen's algorithm, I probably could have convinced you more or less, that there was no way to solve matrix multiplication faster than cubic time. You just look at the definition of the problem and it seems like you have to do cubic work. Yet, Strauss's algorithm and other follow ups show you can do. Fundamentally better, than, the naive cubic running time algorithm, for matrixmultiplication. So there really are, some quite counter-intuitive, and hard to discover, unusually efficient algorithms, within the landscape of polynomial time solutions. And who's to say that there aren't some, more exotic species, in this landscape of polynomial time solvability, that have yet to be discovered, which can make. In-roads even on empty complete problems. At this point, we really don't know. At the very least our currently primitive understanding of the fauna within the complexity class P, is an intimidating obstruction to a proof that P is not equal to NP. I should also mention that, as an interesting counterpoint to Edman's Conjecture in '65, was a conjecture by Gödel. This is the same logician Kurt Gödel, Gödel's completeness and incompleteness theorems. He wrote a letter to John von Neumann in 1956, where he actually conjectured theopposite, the P is = to NP,so who knows. \n",
    "\n",
    "\n",
    "\n",
    "## Algorithmic Approaches to NP-Complete Problems\n",
    "We are now going to focus on algorithmic approaches to NP-complete problems. If you're confronted with an NP complete problem, what should you do about it? <br>\n",
    "So suppose you have identified a computational problem on which thesuccess of your new startup company rests. May be you would spend the last several weeks throwing the kitchen sink at in. All the algorithm design paradigms you know, all the data structures, all the primitives, nothing works. Finally, you decide to try to prove the problem is NPcomplete, and you succeed. Now you have an explanation for why your weeks of effort have come to naught, but that doesn't change the fact that this is the problem that governs the success of your project. What should you do? Well, the good news is, NP completeness is certainly not adeath sentence. There are people solving, or at least approximately solving, NP complete problems all the time. However, knowing that your problem is NP complete does tell you where to set your expectations. You should not expect some general purpose, super-fast algorithm, like we have for other computational problems, like say, sorting, or single source shortest paths. Unless you are dealing with unusually small, or well structured inputs, you're going to have to work pretty hard to solve this problem, and also, possibly, make some compromises. <br>\n",
    "As usual, these general principles should just be a starting point.You should take them, and run with them, augmenting them with whatever domain expertise you have, in the specific problem that you need to solve. <br>\n",
    "\n",
    "Three useful strategies:\n",
    "1. Focus on computationally tractable special cases.\n",
    "The first strategy, is to focus on computationally tractable special cases, of an np complete problem. Related, relatedly you want to think about what's special about your domain or about the data sets that you're working with, and try to understand if there's special structure which can be exploited in your algorithm. Let me point out, we've already done this in a couple of cases in this course. The first example we saw concerns the weighted independent set. So we started this problem on path graphs but computational problem makes perfect sense in general graphs. The general problem is I give you as input, an undirected graph, every vertex has a weight, and I want the maximum weight subset of vertices that is an independent set. And remember, in an independent set, you are forbidden to take any 2 vertices that are neighbors. So in an independent set, none of the pairs of vertices that you've picked are joined by an edge. In general graphs, the way to do an independent set problem is NP complete, so we certainly don't expect it to have a polynomial time algorithm. But, in the special case where the graph is a path, as we saw, there's a linear time, dynamic programming algorithm, that exactly solves the weight of the independent set problem. So path graphs form a special case of the weight of the weighted independent set problem that's computationally tractable, solvable in polynomial, even linear, time. In fact, the frontier of tractability can be pushed well beyond path graphs. the case of graphs that are trees, and notice that you could still do dynamic programming efficiently, to be weighted independent sets and trees. You can even get computationally efficient algorithms for a broader class of graphs, known as bounded tree width graphs. \n",
    "\n",
    "2. Heuristics - fast algorithms -> that are not always correct. \n",
    "So the second strategy, which is certainly one very common in practice, is to resort to heuristics. That is, fast algorithms, which are not guaranteed to be correct. We could classify Carger's randomized minimum cut algorithm as a heuristic, because it did have a small failure probability of failing to find, the minimum cut. \n",
    "\n",
    "3. Solve in exponential time but faster than brute-force search\n",
    " The final strategy, is for situations where you are unwilling to relax correctness. You're unwilling to consider heuristics. Now, of course, for an NP complete problem, if you're always going to be correct, you're not, you don't expect to run in polynomial time, but there are still opportunities to have algorithms that, while exponential time in the worst case, are smarter than naive brute forcesearch. So we have in fact already seen one example that can be interpreted as a implementation of this strategy, that's for the knapsack problem. So, in the knapsack problem, naive brute force search, would just run over all possible subsets of the items. It would check if a subset of items fit in the knapsack. If it does, it remembers the value, and then it just returns the feasible solution. with maximum value. That has time proportional to 2 to the n, where n is the number of items.Our dynamic programming algorithm, has running time n times W. Now, of course, cap- this is no better than 2 to the n, if the knapsack capacity is huge, if it is itself, 2 to the n. But, as we argued, if W is smaller, this algorithm is going to be faster.\n",
    " \n",
    "<table>\n",
    "    <caption>Solve in exponential time but faster than brute-force search </caption>\n",
    "    <tr> <td>$\\textbf{Operation}$ </td> <td>\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{      Running Time  }$ <td> </tr>\n",
    "    <tr> <td> Knapsack Problem</td>\t<td>$O(nW)$ instead of $2^n$<td> </tr>\n",
    "    <tr> <td> TSP </td>\t<td>$2^n$ instead of $n!$<td> </tr>\n",
    "    <tr> <td> Vertex Cover Problem</td>\t<td>$2^{OPT}n$ instead of $n^{OPT}$<td> </tr>\n",
    "</table>\n",
    "\n",
    " \n",
    " \n",
    "***\n",
    "<br>\n",
    "\n",
    "# XXXII. FASTER EXACT ALGORITHMS FOR NP-COMPLETE PROBLEMS \n",
    "\n",
    "## The Vertex Cover Problem\n",
    " This is a well know NP-Complete problem as such we certainly don't expect our exact algorithm to run in polynomial time on general instances. But the algorithm does showcase two of the algorithmic strategies that we discussed for dealing with NP-Complete problems. First of all, we can interpret this algorithm as on general instances. While it's true running in exponential time, never the less, improving markedly over the more obvious naive brute force search. This illustrates the point that for many NP-Complete problems there's a lot of room for algorithmic ingenuity even though you're not going to come up with a polynomial time solution.\n",
    "\n",
    "A second interpretation of the algorithm we're going to develop is that it's polynomial time for special classes of instances. In particular instances that have an unusually good optimal solution.\n",
    "\n",
    "The input is simply an undirected graph.\n",
    "\n",
    "The goal is to identify the smallest size, that is the minimum cardinality of a vertex cover of the graph. What is a vertex cover? We call a subset of the vertices is a vertex cover if for every edge at least one possibly both of the edges end points lie in the set s.\n",
    "\n",
    "There is of course always a feasible solution, you could always choose every single vertex, that's certainly going to be a vertex cover. But the hard question is how do you make sure you get one endpoint from each edge in the most parsimonious way.\n",
    "\n",
    "So what might this problem represent well perhaps your assembling a team of some sorts or maybe a team of programmers maybe of lawyers maybe of football players. Who knows but think of the vertices as being potential people that you could recruit onto your team and think of an edge as representing a potential task that your team might have to complete and the two vertices in the edge represent. The two people who are capable of accomplishing that task. So then what a vertex cover is it says hire enough people so that every single task you can accomplish. You have one of the two people in your team that's capable of accomplishing that task.\n",
    "\n",
    "So what might this problem represent well perhaps your assembling a team of some sorts or maybe a team of programmers maybe of lawyers maybe of football players. Who knows but think of the vertices as being potential people that you could recruit onto your team and think of an edge as representing a potential task that your team might have to complete and the two vertices in the edge represent. The two people who are capable of accomplishing that task. So then what a vertex cover is it says hire enough people so that every single task you can accomplish. You have one of the two people in your team that's capable of accomplishing that task.\n",
    "\n",
    "![The Vertex Cover Problem](images/16_VertexCover.png)\n",
    "\n",
    "The algorithm is recursive. I'm not going to bother to specify the base cases, which is when you're given a graph, with a most 1 vertex. Or whether you're given a value of k, equal to 0 or 1. So let's assume that the graph has a least 2 vertices, and that k is at least equal to 2. I wouldn't be surprised if this algorithm reminds you, of some of the recursive implementations we discussed for various dynamic programming algorithms. And in the same way those algorithms exhibited exponential running time, without memoization, we're going to see an exponential running time here. For all of our dynamic programming algorithms, we were able to cleverly organize sub-problems from smallest to largest, thereby eliminating redundantly solving the same sub-problems over and over again, yielding polynomial run time bounds. Here, we're going to be stuck with the exponential running time bound. Obviously not surprising given Given that it's an NP complete problem, but still, if you really want to understand this deeply, take this recursive search algorithm.Try to come up with a polynomial time version,try to come up with a small set of sub problems, ordered from smallest to largest that you can solve systematically.You will be inevitably stymied in those attempts, but you'll better appreciate what's non trivial about the recursive solution here. In step 1 of the alrogithm, we pick some arbitray edge, u, comma v. Why is it useful to focus on an edge? Well, by the definition of a vertex cover, it has to include either u or v, so that's our initial clue. We're going to proceed optimistically. We're looking for a vertex cover of sizeK, so let's assume one exists. What does the sub structure limit tells us? It says. That if such a solution exists, then necessarily either gu or gv or both themselves have small vertex covers, ofsize only k - 1. Moreover, it's a simple matter to extend such vertex covers to a small 1 for g, you just augment them by the missing vertex. So let's first adopt as a working hypothesis that g sub u has a smaller disk number of size only k-1. Let's recursively try to find it. If our recursive search comes back with a solution of vertex number of size k-1 for g-u, we're good to go. We just augment that by the vertex u and that's our vertex number of size k for the original graph of G If that recursive call fails to find the small vertex cover will you say, oh well then it must be that v is the key to getting a small vertex cover. So we do exactly the same thing, we recursively search g sub v for a vertex cover of size K - 1. If the second recursive call also fails to find a small vertex cover, one of size only k - 1, then by the substructure claim, the other direction, we know that the original graph G cannot have a vertex cover of size k. If it did, the substructure limit tells us. One of the 2 recursive calls would have succeeded. Since they both failed, we conclude correctly that the original graph did not have a small vertex cover.\n",
    "\n",
    "\n",
    "\n",
    "## The Traveling Salesman Problem\n",
    "When we first talked about TSP, it was bad news. The context was NP completeness. We discussed Edmond's Conjecture. That, it's widely believed there's no known polynomial time algorithm for solving the TSP problem. But let's talk about some good news. The fact that you can do better than naive brute-force search. In fact, it's going to be another neat application of the dynamic programming algorithm design. Paradigm. So let me remind you briefly about the traveling salesman problem. The input, very simple, just a complete un-directed graph, and each of the end choose two edges has a non-negative cost. The responsibility of the algorithm is to figure out the minimum cost way of visiting each vertex exactly once, that is you're supposed to output a tour, apermutation on the vertices. That minimizes the sum of the corresponding end edges. For example in this four vertex pink network the minimum cost towards overall cost thirteen. You could of course solve the problem using root four search, the running time of root four search would be in factorial. Tutorial. This would allow you to solve problems with say, 12, 13, maybe 14 vertices. In this video, and the next, we'll develop a dynamic programming algorithm that solves the TSP problem. Now, of course, TSP is NP complete. We have to expect that. We're not expected a polynomial time algorithm. But, this dynamic programming algorithm will run quite a bit faster than brute-force search. The running time will be big O of n^2 times 2^n. 2^n is of course exponential but it's quite a bit better than n factorial. n factorial is more ball park n^n. Okay, if you look at approximation you'll see it's really n over a constant raised to the n but still that's much much bigger than a constant 2 raised to the n. To make it more concrete, you could run this dynamic programming algorithm for values of n, probably pushing 30 or so. So I realize these are still pretty absurdly small problem sizes, compared to the size of the arrrays, that we can sort. Compared to the size of the graphs on which we compute strongly connectedcomponenets, or shortest paths, but, that's how it goes with NP-complete problems. You have to work pretty hard, even to solve modest sized Problems. So at least this dynamic programming outrhyhm proves the point. That even for NP complete problems, there are opportunities to improve over brute-force search and the programmer tool box I've already equipped you with is efficient to make some of those improvements. Increments. \n",
    "\n",
    "\n",
    "![The Traveling Salesman Problem](images/17_TSP.png)\n",
    "\n",
    "The dynamic programming algorithm writes itself. We have a 2 dimensional array. 2 dimensions, 'cuz he have 2 indices, for each sub problem. One specifying the final vertex, j. The other specifying the set capital s. It turns out we can get away with a pretty small set of base cases. We're only going to need to pre-compute the entries of the 2D array, in which the final destination, j=1.\n",
    "\n",
    "It's the same as the started vertex.Now if S happens to contain just the vertex 1, then the empty path goes from 1 to 1 with no intermediate stops that has length 0. Otherwise, if S contains extra vertices there's no way to go from 1 back to 1 with intermediate stops. Visiting the vertex 1 only once. So we define the other set problems with j=1 want to have plus infinity value. Now as usual we saw what the set problem systematically using the recurrence. We want to make sure that rather smaller set problems are solved before the bigger ones, so 1 out of 4 loops we iterate overthe measure of set problem size which member of the carnality of the set s. Amongst sub-problems of this same size, we don't care what order we solve them. So we just iterate through the sets S of  M in arbitrary order. So these outer two for loops accomplish the following, they iterate through the relevant choices of S. That is sets S to have cardinality at least 2 and contain the vertex of 1. So that's the first index of our 2D array. What about the 2nd index J, so this is where the shortest path in a given sub. The problem is supposed to stop.Well subproblems only make sense, are only defined if that final vertex J, is one of the vertices S, that you are supposed to visit.So when we get to a given choice of capital S, at that point we are going to iterate through all the relevant choices of j.and if you recall our argument in the base case and the fact the this SS has size at east 2, there is no point in trying j=1. So now that we have a subproblem. We have our choice of S, we have our choice of J. We just compute the recurrence for this particular subproblem. So what is that? Well, we take a best case choice of the penultimate vertex. So that's some vertex that we've got to a visit. A vertex k and capital S. Other than the one where we stopped, so it can't be equal to j. And then, given a choice of k, what is the corresponding solution of that candidate path from 1 to j. Well, it's whatever the shortest path isfrom 1 to k. Which of course is visiting everything in s, except for j. Plus whatever the cost of the final hop is from K to J, so the recurrence just figures out what is the optimal choice of K by brute-force search and then by our optimal substructure lemma we know that's the correct answer to this sub problem. Now in almost all of our dynamic programming algorithms, after we solved for the sub problems, all we did was return the value of the biggest one. Here we actually have to do a tiny bit of extra work. It is not the case that the solution we care about. The minimum cost traveling salesman tour is literally one of the sub problems. However, it is true that we can easilycompute the minimum cost of a traveling salesman tour given solutions to all of the sub problems. So what is the biggest sub problem? Well there's actually n of them. Remember our measure of sub problem size is the cardinality of the set S, the number of vertices that you've got to visit in that In that sub problem. So in the biggest sub problems S is equal to everything, you have to compute a shortest path that visits each of the N vertices exactly once. So 1 are the semantics of 1 of those sub problems, for a given choice of the 2nd index J, there's N different choices for that final vertex J. That sub problem was responsible for computing the length of the shortest path. Its starts at the vertex 1 it concludes at the vertex J, and it visits every single vertex exactly once. Now hows is this different than a travel salesman tour? Well all thats missing is that finally hop, that hop from J to 1. So what this means is that, after we've solved all of the subproblems in our triple 4 loop. We can complete the algorithm with 1 final brute-force search. This final time that brute-force search is over the choice of J. The last vertex that you visit before you return home to vertex #1. So you can skip the choice of J, for 1, that 1 doesn't make sense. But for the other N-1 possibilities for that final vertex J, you take the previously computed value of the shortest path, it starts at 1 and goes to J. Exactly once, you tack on to that the cost of the final hop, going home to 1 from vertex j, and of those n-1 different solutions, you return the best of 'em. That is in fact the cost of the, minimum cost travelling salesman tour. The correctness argument is the same as it always is. The correctness of the optimal substructure lemma Implies the correctness of the recurrence and then the correctness of the dynamic programming algorithm follows from the correctness of the recurrence plus induction. The running time is equally straight forward. So because of the sub problems we're keeping track of a lot of information. Specifically the identities of all the intermediate vertices. There are a lot of sub problems. So in particular there's roughly 2^n choices for the set capital S, there's roughly n choices, for the final destination j. So combining those, we get n*2^n sub problems. How much work do you do per sub-problem? Well you have to execute this brute-force search over the chase, choices of the vertex k. The second to last vertex on the path, k can be almost anything in the set capital S, capital S could have linear size. So you're going to do o(n) work per sub-problem.Thus, the overall amount of work, as promised is O of n^2*2n^2. This is still exponential time, there's still sever limits on how big a value of n you're going to be able to execute this algorithm for. That said, the problem is mp complete and you have to respect its computationalintractability. And again, at least this shows there are often interesting algorithmic opportu, opportunities to beat brute-force search, even for mp complete problems like the traffic salesman.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
